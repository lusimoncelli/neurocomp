{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image, masking, plotting\n",
    "from nilearn.decoding import SearchLight\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import spacy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shared Response Model (SRM)\n",
    "\n",
    "The implementations are based on the following publications:\n",
    "\n",
    ".. [Chen2015] \"A Reduced-Dimension fMRI Shared Response Model\",\n",
    "   P.-H. Chen, J. Chen, Y. Yeshurun-Dishon, U. Hasson, J. Haxby, P. Ramadge\n",
    "   Advances in Neural Information Processing Systems (NIPS), 2015.\n",
    "   http://papers.nips.cc/paper/5855-a-reduced-dimension-fmri-shared-response-model\n",
    "\n",
    ".. [Anderson2016] \"Enabling Factor Analysis on Thousand-Subject Neuroimaging\n",
    "   Datasets\",\n",
    "   Michael J. Anderson, Mihai CapotÄƒ, Javier S. Turek, Xia Zhu, Theodore L.\n",
    "   Willke, Yida Wang, Po-Hsuan Chen, Jeremy R. Manning, Peter J. Ramadge,\n",
    "   Kenneth A. Norman,\n",
    "   IEEE International Conference on Big Data, 2016.\n",
    "   https://doi.org/10.1109/BigData.2016.7840719\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Po-Hsuan Chen (Princeton Neuroscience Institute) and Javier Turek\n",
    "# (Intel Labs), 2015\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import assert_all_finite\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "__all__ = [\n",
    "    \"DetSRM\",\n",
    "    \"SRM\",\n",
    "]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _init_w_transforms(data, features, random_states, comm=MPI.COMM_SELF):\n",
    "    \"\"\"Initialize the mappings (Wi) for the SRM with random orthogonal\n",
    "    matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    data : list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "        Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "    features : int\n",
    "        The number of features in the model.\n",
    "\n",
    "    random_states : list of `RandomState`s\n",
    "        One `RandomState` instance per subject.\n",
    "\n",
    "    comm : mpi4py.MPI.Intracomm\n",
    "        The MPI communicator containing the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    w : list of array, element i has shape=[voxels_i, features]\n",
    "        The initialized orthogonal transforms (mappings) :math:`W_i` for each\n",
    "        subject.\n",
    "\n",
    "    voxels : list of int\n",
    "        A list with the number of voxels per subject.\n",
    "\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "\n",
    "        This function assumes that the numpy random number generator was\n",
    "        initialized.\n",
    "\n",
    "        Not thread safe.\n",
    "    \"\"\"\n",
    "    w = []\n",
    "    subjects = len(data)\n",
    "    voxels = np.empty(subjects, dtype=int)\n",
    "\n",
    "    # Set Wi to a random orthogonal voxels by features matrix\n",
    "    for subject in range(subjects):\n",
    "        if data[subject] is not None:\n",
    "            voxels[subject] = data[subject].shape[0]\n",
    "            rnd_matrix = random_states[subject].random_sample((\n",
    "                voxels[subject], features))\n",
    "            q, r = np.linalg.qr(rnd_matrix)\n",
    "            w.append(q)\n",
    "        else:\n",
    "            voxels[subject] = 0\n",
    "            w.append(None)\n",
    "    voxels = comm.allreduce(voxels, op=MPI.SUM)\n",
    "    return w, voxels\n",
    "\n",
    "\n",
    "def load(file):\n",
    "    \"\"\"Load fitted SRM from .npz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    file : str, file-like object, or pathlib.Path\n",
    "        The .npz file to read containing fitted SRM saved using srm.save\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "\n",
    "    srm : fitted SRM model\n",
    "    \"\"\"\n",
    "\n",
    "    # Load file and extract SRM attributes\n",
    "    loaded = np.load(file)\n",
    "    w_ = [s for s in loaded['w_']]\n",
    "    s_ = loaded['s_']\n",
    "    sigma_s_ = loaded['sigma_s_']\n",
    "    mu_ = [s for s in loaded['mu_']]\n",
    "    rho2_ = loaded['rho2_']\n",
    "    features, n_iter, rand_seed = loaded['kwargs']\n",
    "\n",
    "    # Initialize new SRM object and attach loaded attributes\n",
    "    srm = SRM(n_iter=n_iter, features=features, rand_seed=rand_seed)\n",
    "    srm.w_ = w_\n",
    "    srm.s_ = s_\n",
    "    srm.sigma_s_ = sigma_s_\n",
    "    srm.mu_ = mu_\n",
    "    srm.rho2_ = rho2_\n",
    "\n",
    "    return srm\n",
    "\n",
    "\n",
    "class SRM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Probabilistic Shared Response Model (SRM)\n",
    "\n",
    "    Given multi-subject data, factorize it as a shared response S among all\n",
    "    subjects and an orthogonal transform W per subject:\n",
    "\n",
    "    .. math:: X_i \\\\approx W_i S, \\\\forall i=1 \\\\dots N\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    n_iter : int, default: 10\n",
    "        Number of iterations to run the algorithm.\n",
    "\n",
    "    features : int, default: 50\n",
    "        Number of features to compute.\n",
    "\n",
    "    rand_seed : int, default: 0\n",
    "        Seed for initializing the random number generator.\n",
    "\n",
    "    comm : mpi4py.MPI.Intracomm\n",
    "        The MPI communicator containing the data\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    w_ : list of array, element i has shape=[voxels_i, features]\n",
    "        The orthogonal transforms (mappings) for each subject.\n",
    "\n",
    "    s_ : array, shape=[features, samples]\n",
    "        The shared response.\n",
    "\n",
    "    sigma_s_ : array, shape=[features, features]\n",
    "        The covariance of the shared response Normal distribution.\n",
    "\n",
    "    mu_ : list of array, element i has shape=[voxels_i]\n",
    "        The voxel means over the samples for each subject.\n",
    "\n",
    "    rho2_ : array, shape=[subjects]\n",
    "        The estimated noise variance :math:`\\\\rho_i^2` for each subject\n",
    "\n",
    "    comm : mpi4py.MPI.Intracomm\n",
    "        The MPI communicator containing the data\n",
    "\n",
    "    random_state_: `RandomState`\n",
    "        Random number generator initialized using rand_seed\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "\n",
    "       The number of voxels may be different between subjects. However, the\n",
    "       number of samples must be the same across subjects.\n",
    "\n",
    "       The probabilistic Shared Response Model is approximated using the\n",
    "       Expectation Maximization (EM) algorithm proposed in [Chen2015]_. The\n",
    "       implementation follows the optimizations published in [Anderson2016]_.\n",
    "\n",
    "       This is a single node version.\n",
    "\n",
    "       The run-time complexity is :math:`O(I (V T K + V K^2 + K^3))` and the\n",
    "       memory complexity is :math:`O(V T)` with I - the number of iterations,\n",
    "       V - the sum of voxels from all subjects, T - the number of samples, and\n",
    "       K - the number of features (typically, :math:`V \\\\gg T \\\\gg K`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=10, features=50, rand_seed=0,\n",
    "                 comm=MPI.COMM_SELF):\n",
    "        self.n_iter = n_iter\n",
    "        self.features = features\n",
    "        self.rand_seed = rand_seed\n",
    "        self.comm = comm\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the probabilistic Shared Response Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X :  list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "        y : not used\n",
    "        \"\"\"\n",
    "        logger.info('Starting Probabilistic SRM')\n",
    "\n",
    "        # Check the number of subjects\n",
    "        if len(X) <= 1:\n",
    "            raise ValueError(\"There are not enough subjects \"\n",
    "                             \"({0:d}) to train the model.\".format(len(X)))\n",
    "\n",
    "        # Check for input data sizes\n",
    "        number_subjects = len(X)\n",
    "        number_subjects_vec = self.comm.allgather(number_subjects)\n",
    "        for rank in range(self.comm.Get_size()):\n",
    "            if number_subjects_vec[rank] != number_subjects:\n",
    "                raise ValueError(\n",
    "                    \"Not all ranks have same number of subjects\")\n",
    "\n",
    "        # Collect size information\n",
    "        shape0 = np.zeros((number_subjects,), dtype=int)\n",
    "        shape1 = np.zeros((number_subjects,), dtype=int)\n",
    "\n",
    "        for subject in range(number_subjects):\n",
    "            if X[subject] is not None:\n",
    "                assert_all_finite(X[subject])\n",
    "                shape0[subject] = X[subject].shape[0]\n",
    "                shape1[subject] = X[subject].shape[1]\n",
    "\n",
    "        shape0 = self.comm.allreduce(shape0, op=MPI.SUM)\n",
    "        shape1 = self.comm.allreduce(shape1, op=MPI.SUM)\n",
    "\n",
    "        # Check if all subjects have same number of TRs\n",
    "        number_trs = np.min(shape1)\n",
    "        for subject in range(number_subjects):\n",
    "            if shape1[subject] < self.features:\n",
    "                raise ValueError(\n",
    "                    \"There are not enough samples to train the model with \"\n",
    "                    \"{0:d} features.\".format(self.features))\n",
    "            if shape1[subject] != number_trs:\n",
    "                raise ValueError(\"Different number of samples between subjects\"\n",
    "                                 \".\")\n",
    "        # Run SRM\n",
    "        self.sigma_s_, self.w_, self.mu_, self.rho2_, self.s_ = self._srm(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Use the model to transform matrix to Shared Response space\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n",
    "            Each element in the list contains the fMRI data of one subject\n",
    "            note that number of voxels and samples can vary across subjects\n",
    "        y : not used (as it is unsupervised learning)\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s : list of 2D arrays, element i has shape=[features_i, samples_i]\n",
    "            Shared responses from input data (X)\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the model exist\n",
    "        if hasattr(self, 'w_') is False:\n",
    "            raise NotFittedError(\"The model fit has not been run yet.\")\n",
    "\n",
    "        # Check the number of subjects\n",
    "        if len(X) != len(self.w_):\n",
    "            raise ValueError(\"The number of subjects does not match the one\"\n",
    "                             \" in the model.\")\n",
    "\n",
    "        s = [None] * len(X)\n",
    "        for subject in range(len(X)):\n",
    "            if X[subject] is not None:\n",
    "                s[subject] = self.w_[subject].T.dot(X[subject])\n",
    "\n",
    "        return s\n",
    "\n",
    "    def _init_structures(self, data, subjects):\n",
    "        \"\"\"Initializes data structures for SRM and preprocess the data.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "        subjects : int\n",
    "            The total number of subjects in `data`.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : list of array, element i has shape=[voxels_i, samples]\n",
    "            Demeaned data for each subject.\n",
    "\n",
    "        mu : list of array, element i has shape=[voxels_i]\n",
    "            Voxel means over samples, per subject.\n",
    "\n",
    "        rho2 : array, shape=[subjects]\n",
    "            Noise variance :math:`\\\\rho^2` per subject.\n",
    "\n",
    "        trace_xtx : array, shape=[subjects]\n",
    "            The squared Frobenius norm of the demeaned data in `x`.\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        mu = []\n",
    "        rho2 = np.zeros(subjects)\n",
    "\n",
    "        trace_xtx = np.zeros(subjects)\n",
    "\n",
    "        for subject in range(subjects):\n",
    "            rho2[subject] = 1\n",
    "            if data[subject] is not None:\n",
    "                mu.append(np.mean(data[subject], 1))\n",
    "                trace_xtx[subject] = np.sum(data[subject] ** 2)\n",
    "                x.append(data[subject] - mu[subject][:, np.newaxis])\n",
    "            else:\n",
    "                mu.append(None)\n",
    "                trace_xtx[subject] = 0\n",
    "                x.append(None)\n",
    "\n",
    "        return x, mu, rho2, trace_xtx\n",
    "\n",
    "    def _likelihood(self, chol_sigma_s_rhos, log_det_psi, chol_sigma_s,\n",
    "                    trace_xt_invsigma2_x, inv_sigma_s_rhos, wt_invpsi_x,\n",
    "                    samples):\n",
    "        \"\"\"Calculate the log-likelihood function\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        chol_sigma_s_rhos : array, shape=[features, features]\n",
    "            Cholesky factorization of the matrix (Sigma_S + sum_i(1/rho_i^2)\n",
    "            * I)\n",
    "\n",
    "        log_det_psi : float\n",
    "            Determinant of diagonal matrix Psi (containing the rho_i^2 value\n",
    "            voxels_i times).\n",
    "\n",
    "        chol_sigma_s : array, shape=[features, features]\n",
    "            Cholesky factorization of the matrix Sigma_S\n",
    "\n",
    "        trace_xt_invsigma2_x : float\n",
    "            Trace of :math:`\\\\sum_i (||X_i||_F^2/\\\\rho_i^2)`\n",
    "\n",
    "        inv_sigma_s_rhos : array, shape=[features, features]\n",
    "            Inverse of :math:`(\\\\Sigma_S + \\\\sum_i(1/\\\\rho_i^2) * I)`\n",
    "\n",
    "        wt_invpsi_x : array, shape=[features, samples]\n",
    "\n",
    "        samples : int\n",
    "            The total number of samples in the data.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        loglikehood : float\n",
    "            The log-likelihood value.\n",
    "        \"\"\"\n",
    "        log_det = (np.log(np.diag(chol_sigma_s_rhos) ** 2).sum() + log_det_psi\n",
    "                   + np.log(np.diag(chol_sigma_s) ** 2).sum())\n",
    "        loglikehood = -0.5 * samples * log_det - 0.5 * trace_xt_invsigma2_x\n",
    "        loglikehood += 0.5 * np.trace(\n",
    "            wt_invpsi_x.T.dot(inv_sigma_s_rhos).dot(wt_invpsi_x))\n",
    "        # + const --> -0.5*nTR*nvoxel*subjects*math.log(2*math.pi)\n",
    "\n",
    "        return loglikehood\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_transform_subject(Xi, S):\n",
    "        \"\"\"Updates the mappings `W_i` for one subject.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Xi : array, shape=[voxels, timepoints]\n",
    "            The fMRI data :math:`X_i` for aligning the subject.\n",
    "\n",
    "        S : array, shape=[features, timepoints]\n",
    "            The shared response.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        Wi : array, shape=[voxels, features]\n",
    "            The orthogonal transform (mapping) :math:`W_i` for the subject.\n",
    "        \"\"\"\n",
    "        A = Xi.dot(S.T)\n",
    "        # Solve the Procrustes problem\n",
    "        U, _, V = np.linalg.svd(A, full_matrices=False)\n",
    "        return U.dot(V)\n",
    "\n",
    "    def transform_subject(self, X):\n",
    "        \"\"\"Transform a new subject using the existing model.\n",
    "        The subject is assumed to have recieved equivalent stimulation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : 2D array, shape=[voxels, timepoints]\n",
    "            The fMRI data of the new subject.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        w : 2D array, shape=[voxels, features]\n",
    "            Orthogonal mapping `W_{new}` for new subject\n",
    "\n",
    "        \"\"\"\n",
    "        # Check if the model exist\n",
    "        if hasattr(self, 'w_') is False:\n",
    "            raise NotFittedError(\"The model fit has not been run yet.\")\n",
    "\n",
    "        # Check the number of TRs in the subject\n",
    "        if X.shape[1] != self.s_.shape[1]:\n",
    "            raise ValueError(\"The number of timepoints(TRs) does not match the\"\n",
    "                             \"one in the model.\")\n",
    "\n",
    "        w = self._update_transform_subject(X, self.s_)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def save(self, file):\n",
    "        \"\"\"Save fitted SRM to .npz file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        file : str, file-like object, or pathlib.Path\n",
    "            Filename (string), open file (file-like object) or pathlib.Path\n",
    "            where the fitted SRM will be saved. If file is a string or a Path,\n",
    "            the .npz extension will be appended to the filename if it is not\n",
    "            already there.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the model has been estimated\n",
    "        if hasattr(self, 'w_') is False:\n",
    "            raise NotFittedError(\"The model fit has not been run yet.\")\n",
    "\n",
    "        np.savez_compressed(\n",
    "            file,\n",
    "            w_=self.w_,\n",
    "            s_=self.s_,\n",
    "            sigma_s_=self.sigma_s_,\n",
    "            mu_=self.mu_,\n",
    "            rho2_=self.rho2_,\n",
    "            kwargs=np.array([self.features, self.n_iter, self.rand_seed])\n",
    "        )\n",
    "\n",
    "    def _srm(self, data):\n",
    "        \"\"\"Expectation-Maximization algorithm for fitting the probabilistic\n",
    "        SRM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data : list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        sigma_s : array, shape=[features, features]\n",
    "            The covariance :math:`\\\\Sigma_s` of the shared response Normal\n",
    "            distribution.\n",
    "\n",
    "        w : list of array, element i has shape=[voxels_i, features]\n",
    "            The orthogonal transforms (mappings) :math:`W_i` for each subject.\n",
    "\n",
    "        mu : list of array, element i has shape=[voxels_i]\n",
    "            The voxel means :math:`\\\\mu_i` over the samples for each subject.\n",
    "\n",
    "        rho2 : array, shape=[subjects]\n",
    "            The estimated noise variance :math:`\\\\rho_i^2` for each subject\n",
    "\n",
    "        s : array, shape=[features, samples]\n",
    "            The shared response.\n",
    "        \"\"\"\n",
    "\n",
    "        local_min = min([d.shape[1] for d in data if d is not None],\n",
    "                        default=sys.maxsize)\n",
    "        samples = self.comm.allreduce(local_min, op=MPI.MIN)\n",
    "        subjects = len(data)\n",
    "        self.random_state_ = np.random.RandomState(self.rand_seed)\n",
    "        random_states = [\n",
    "            np.random.RandomState(self.random_state_.randint(2 ** 32))\n",
    "            for i in range(len(data))]\n",
    "\n",
    "        # Initialization step: initialize the outputs with initial values,\n",
    "        # voxels with the number of voxels in each subject, and trace_xtx with\n",
    "        # the ||X_i||_F^2 of each subject.\n",
    "        w, voxels = _init_w_transforms(data, self.features, random_states,\n",
    "                                       self.comm)\n",
    "        x, mu, rho2, trace_xtx = self._init_structures(data, subjects)\n",
    "        shared_response = np.zeros((self.features, samples))\n",
    "        sigma_s = np.identity(self.features)\n",
    "\n",
    "        rank = self.comm.Get_rank()\n",
    "\n",
    "        # Main loop of the algorithm (run\n",
    "        for iteration in range(self.n_iter):\n",
    "            logger.info('Iteration %d' % (iteration + 1))\n",
    "\n",
    "            # E-step:\n",
    "\n",
    "            # Sum the inverted the rho2 elements for computing W^T * Psi^-1 * W\n",
    "            if rank == 0:\n",
    "                rho0 = (1 / rho2).sum()\n",
    "\n",
    "                # Invert Sigma_s using Cholesky factorization\n",
    "                (chol_sigma_s, lower_sigma_s) = scipy.linalg.cho_factor(\n",
    "                    sigma_s, check_finite=False)\n",
    "                inv_sigma_s = scipy.linalg.cho_solve(\n",
    "                    (chol_sigma_s, lower_sigma_s), np.identity(self.features),\n",
    "                    check_finite=False)\n",
    "\n",
    "                # Invert (Sigma_s + rho_0 * I) using Cholesky factorization\n",
    "                sigma_s_rhos = inv_sigma_s + np.identity(self.features) * rho0\n",
    "                chol_sigma_s_rhos, lower_sigma_s_rhos = \\\n",
    "                    scipy.linalg.cho_factor(sigma_s_rhos,\n",
    "                                            check_finite=False)\n",
    "                inv_sigma_s_rhos = scipy.linalg.cho_solve(\n",
    "                    (chol_sigma_s_rhos, lower_sigma_s_rhos),\n",
    "                    np.identity(self.features), check_finite=False)\n",
    "\n",
    "            # Compute the sum of W_i^T * rho_i^-2 * X_i, and the sum of traces\n",
    "            # of X_i^T * rho_i^-2 * X_i\n",
    "            wt_invpsi_x = np.zeros((self.features, samples))\n",
    "            trace_xt_invsigma2_x = 0.0\n",
    "            for subject in range(subjects):\n",
    "                if data[subject] is not None:\n",
    "                    wt_invpsi_x += (w[subject].T.dot(x[subject])) \\\n",
    "                                   / rho2[subject]\n",
    "                    trace_xt_invsigma2_x += trace_xtx[subject] / rho2[subject]\n",
    "\n",
    "            wt_invpsi_x = self.comm.reduce(wt_invpsi_x, op=MPI.SUM)\n",
    "            trace_xt_invsigma2_x = self.comm.reduce(trace_xt_invsigma2_x,\n",
    "                                                    op=MPI.SUM)\n",
    "            trace_sigma_s = None\n",
    "            if rank == 0:\n",
    "                log_det_psi = np.sum(np.log(rho2) * voxels)\n",
    "\n",
    "                # Update the shared response\n",
    "                shared_response = sigma_s.dot(\n",
    "                    np.identity(self.features) - rho0 * inv_sigma_s_rhos).dot(\n",
    "                    wt_invpsi_x)\n",
    "\n",
    "                # M-step\n",
    "\n",
    "                # Update Sigma_s and compute its trace\n",
    "                sigma_s = (inv_sigma_s_rhos\n",
    "                           + shared_response.dot(shared_response.T) / samples)\n",
    "                trace_sigma_s = samples * np.trace(sigma_s)\n",
    "\n",
    "            shared_response = self.comm.bcast(shared_response)\n",
    "            trace_sigma_s = self.comm.bcast(trace_sigma_s)\n",
    "\n",
    "            # Update each subject's mapping transform W_i and error variance\n",
    "            # rho_i^2\n",
    "            for subject in range(subjects):\n",
    "                if x[subject] is not None:\n",
    "                    a_subject = x[subject].dot(shared_response.T)\n",
    "                    perturbation = np.zeros(a_subject.shape)\n",
    "                    np.fill_diagonal(perturbation, 0.001)\n",
    "                    u_subject, s_subject, v_subject = np.linalg.svd(\n",
    "                        a_subject + perturbation, full_matrices=False)\n",
    "                    w[subject] = u_subject.dot(v_subject)\n",
    "                    rho2[subject] = trace_xtx[subject]\n",
    "                    rho2[subject] += -2 * np.sum(w[subject] * a_subject).sum()\n",
    "                    rho2[subject] += trace_sigma_s\n",
    "                    rho2[subject] /= samples * voxels[subject]\n",
    "                else:\n",
    "                    rho2[subject] = 0\n",
    "\n",
    "            rho2 = self.comm.allreduce(rho2, op=MPI.SUM)\n",
    "\n",
    "            if rank == 0:\n",
    "                if logger.isEnabledFor(logging.INFO):\n",
    "                    # Calculate and log the current log-likelihood for checking\n",
    "                    # convergence\n",
    "                    loglike = self._likelihood(\n",
    "                        chol_sigma_s_rhos, log_det_psi, chol_sigma_s,\n",
    "                        trace_xt_invsigma2_x, inv_sigma_s_rhos, wt_invpsi_x,\n",
    "                        samples)\n",
    "                    logger.info('Objective function %f' % loglike)\n",
    "\n",
    "        sigma_s = self.comm.bcast(sigma_s)\n",
    "        return sigma_s, w, mu, rho2, shared_response\n",
    "\n",
    "\n",
    "class DetSRM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Deterministic Shared Response Model (DetSRM)\n",
    "\n",
    "    Given multi-subject data, factorize it as a shared response S among all\n",
    "    subjects and an orthogonal transform W per subject:\n",
    "\n",
    "    .. math:: X_i \\\\approx W_i S, \\\\forall i=1 \\\\dots N\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    n_iter : int, default: 10\n",
    "        Number of iterations to run the algorithm.\n",
    "\n",
    "    features : int, default: 50\n",
    "        Number of features to compute.\n",
    "\n",
    "    rand_seed : int, default: 0\n",
    "        Seed for initializing the random number generator.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    w_ : list of array, element i has shape=[voxels_i, features]\n",
    "        The orthogonal transforms (mappings) for each subject.\n",
    "\n",
    "    s_ : array, shape=[features, samples]\n",
    "        The shared response.\n",
    "\n",
    "    random_state_: `RandomState`\n",
    "        Random number generator initialized using rand_seed\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "\n",
    "        The number of voxels may be different between subjects. However, the\n",
    "        number of samples must be the same across subjects.\n",
    "\n",
    "        The Deterministic Shared Response Model is approximated using the\n",
    "        Block Coordinate Descent (BCD) algorithm proposed in [Chen2015]_.\n",
    "\n",
    "        This is a single node version.\n",
    "\n",
    "        The run-time complexity is :math:`O(I (V T K + V K^2))` and the memory\n",
    "        complexity is :math:`O(V T)` with I - the number of iterations, V - the\n",
    "        sum of voxels from all subjects, T - the number of samples, K - the\n",
    "        number of features (typically, :math:`V \\\\gg T \\\\gg K`), and N - the\n",
    "        number of subjects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=10, features=50, rand_seed=0):\n",
    "        self.n_iter = n_iter\n",
    "        self.features = features\n",
    "        self.rand_seed = rand_seed\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the Deterministic Shared Response Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "        y : not used\n",
    "        \"\"\"\n",
    "        logger.info('Starting Deterministic SRM')\n",
    "\n",
    "        # Check the number of subjects\n",
    "        if len(X) <= 1:\n",
    "            raise ValueError(\"There are not enough subjects \"\n",
    "                             \"({0:d}) to train the model.\".format(len(X)))\n",
    "\n",
    "        # Check for input data sizes\n",
    "        if X[0].shape[1] < self.features:\n",
    "            raise ValueError(\n",
    "                \"There are not enough samples to train the model with \"\n",
    "                \"{0:d} features.\".format(self.features))\n",
    "\n",
    "        # Check if all subjects have same number of TRs\n",
    "        number_trs = X[0].shape[1]\n",
    "        number_subjects = len(X)\n",
    "        for subject in range(number_subjects):\n",
    "            assert_all_finite(X[subject])\n",
    "            if X[subject].shape[1] != number_trs:\n",
    "                raise ValueError(\"Different number of samples between subjects\"\n",
    "                                 \".\")\n",
    "\n",
    "        # Run SRM\n",
    "        self.w_, self.s_ = self._srm(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Use the model to transform data to the Shared Response subspace\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "        y : not used\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s : list of 2D arrays, element i has shape=[features_i, samples_i]\n",
    "            Shared responses from input data (X)\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the model exist\n",
    "        if hasattr(self, 'w_') is False:\n",
    "            raise NotFittedError(\"The model fit has not been run yet.\")\n",
    "\n",
    "        # Check the number of subjects\n",
    "        if len(X) != len(self.w_):\n",
    "            raise ValueError(\"The number of subjects does not match the one\"\n",
    "                             \" in the model.\")\n",
    "\n",
    "        s = [None] * len(X)\n",
    "        for subject in range(len(X)):\n",
    "            s[subject] = self.w_[subject].T.dot(X[subject])\n",
    "\n",
    "        return s\n",
    "\n",
    "    def _objective_function(self, data, w, s):\n",
    "        \"\"\"Calculate the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data : list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "        w : list of 2D arrays, element i has shape=[voxels_i, features]\n",
    "            The orthogonal transforms (mappings) :math:`W_i` for each subject.\n",
    "\n",
    "        s : array, shape=[features, samples]\n",
    "            The shared response\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        objective : float\n",
    "            The objective function value.\n",
    "        \"\"\"\n",
    "        subjects = len(data)\n",
    "        objective = 0.0\n",
    "        for m in range(subjects):\n",
    "            objective += \\\n",
    "                np.linalg.norm(data[m] - w[m].dot(s), 'fro') ** 2\n",
    "\n",
    "        return objective * 0.5 / data[0].shape[1]\n",
    "\n",
    "    def _compute_shared_response(self, data, w):\n",
    "        \"\"\" Compute the shared response S\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data : list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "        w : list of 2D arrays, element i has shape=[voxels_i, features]\n",
    "            The orthogonal transforms (mappings) :math:`W_i` for each subject.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        s : array, shape=[features, samples]\n",
    "            The shared response for the subjects data with the mappings in w.\n",
    "        \"\"\"\n",
    "        s = np.zeros((w[0].shape[1], data[0].shape[1]))\n",
    "        for m in range(len(w)):\n",
    "            s = s + w[m].T.dot(data[m])\n",
    "        s /= len(w)\n",
    "\n",
    "        return s\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_transform_subject(Xi, S):\n",
    "        \"\"\"Updates the mappings `W_i` for one subject.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Xi : array, shape=[voxels, timepoints]\n",
    "            The fMRI data :math:`X_i` for aligning the subject.\n",
    "\n",
    "        S : array, shape=[features, timepoints]\n",
    "            The shared response.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        Wi : array, shape=[voxels, features]\n",
    "            The orthogonal transform (mapping) :math:`W_i` for the subject.\n",
    "        \"\"\"\n",
    "        A = Xi.dot(S.T)\n",
    "        # Solve the Procrustes problem\n",
    "        U, _, V = np.linalg.svd(A, full_matrices=False)\n",
    "        return U.dot(V)\n",
    "\n",
    "    def transform_subject(self, X):\n",
    "        \"\"\"Transform a new subject using the existing model.\n",
    "        The subject is assumed to have recieved equivalent stimulation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        X : 2D array, shape=[voxels, timepoints]\n",
    "            The fMRI data of the new subject.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        w : 2D array, shape=[voxels, features]\n",
    "            Orthogonal mapping `W_{new}` for new subject\n",
    "        \"\"\"\n",
    "        # Check if the model exist\n",
    "        if hasattr(self, 'w_') is False:\n",
    "            raise NotFittedError(\"The model fit has not been run yet.\")\n",
    "\n",
    "        # Check the number of TRs in the subject\n",
    "        if X.shape[1] != self.s_.shape[1]:\n",
    "            raise ValueError(\"The number of timepoints(TRs) does not match the\"\n",
    "                             \"one in the model.\")\n",
    "\n",
    "        w = self._update_transform_subject(X, self.s_)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def _srm(self, data):\n",
    "        \"\"\"Expectation-Maximization algorithm for fitting the probabilistic\n",
    "        SRM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data : list of 2D arrays, element i has shape=[voxels_i, samples]\n",
    "            Each element in the list contains the fMRI data of one subject.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        w : list of array, element i has shape=[voxels_i, features]\n",
    "            The orthogonal transforms (mappings) :math:`W_i` for each subject.\n",
    "\n",
    "        s : array, shape=[features, samples]\n",
    "            The shared response.\n",
    "        \"\"\"\n",
    "\n",
    "        subjects = len(data)\n",
    "\n",
    "        self.random_state_ = np.random.RandomState(self.rand_seed)\n",
    "        random_states = [\n",
    "            np.random.RandomState(self.random_state_.randint(2 ** 32))\n",
    "            for i in range(len(data))]\n",
    "\n",
    "        # Initialization step: initialize the outputs with initial values,\n",
    "        # voxels with the number of voxels in each subject.\n",
    "        w, _ = _init_w_transforms(data, self.features, random_states)\n",
    "        shared_response = self._compute_shared_response(data, w)\n",
    "        if logger.isEnabledFor(logging.INFO):\n",
    "            # Calculate the current objective function value\n",
    "            objective = self._objective_function(data, w, shared_response)\n",
    "            logger.info('Objective function %f' % objective)\n",
    "\n",
    "        # Main loop of the algorithm\n",
    "        for iteration in range(self.n_iter):\n",
    "            logger.info('Iteration %d' % (iteration + 1))\n",
    "\n",
    "            # Update each subject's mapping transform W_i:\n",
    "            for subject in range(subjects):\n",
    "                a_subject = data[subject].dot(shared_response.T)\n",
    "                perturbation = np.zeros(a_subject.shape)\n",
    "                np.fill_diagonal(perturbation, 0.001)\n",
    "                u_subject, _, v_subject = np.linalg.svd(\n",
    "                    a_subject + perturbation, full_matrices=False)\n",
    "                w[subject] = u_subject.dot(v_subject)\n",
    "\n",
    "            # Update the shared response:\n",
    "            shared_response = self._compute_shared_response(data, w)\n",
    "\n",
    "            if logger.isEnabledFor(logging.INFO):\n",
    "                # Calculate the current objective function value\n",
    "                objective = self._objective_function(data, w, shared_response)\n",
    "                logger.info('Objective function %f' % objective)\n",
    "\n",
    "        return w, shared_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI data succesfully loaded\n",
      "Shape of one example voxel matrix: (80, 80, 35, 441)\n"
     ]
    }
   ],
   "source": [
    "# carga de archivos fmri\n",
    "data_dir = 'C:/Users/lusim/OneDrive/Desktop/neurocomp/ds000113'\n",
    "\n",
    "fnames_data = [ f'{data_dir}/sub-01/ses-movie/func/sub-01_ses-movie_task-movie_run-{i}_bold.nii.gz' for i in range(1,9)]\n",
    "\n",
    "fmri_imgs = []\n",
    "fmri_data = []\n",
    "\n",
    "for f in fnames_data:\n",
    "    img = nib.load(f)\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    fmri_imgs.append(img)\n",
    "    fmri_data.append(data)\n",
    "    \n",
    "print('fMRI data succesfully loaded')\n",
    "print(f'Shape of one example voxel matrix: {fmri_imgs[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80, 35, 451)\n",
      "(80, 80, 35, 441)\n",
      "(80, 80, 35, 438)\n",
      "(80, 80, 35, 488)\n",
      "(80, 80, 35, 462)\n",
      "(80, 80, 35, 439)\n",
      "(80, 80, 35, 542)\n",
      "(80, 80, 35, 338)\n"
     ]
    }
   ],
   "source": [
    "for array in fmri_data:\n",
    "    print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80, 35, 3599)\n"
     ]
    }
   ],
   "source": [
    "complete_movie_run_sub1 = np.concatenate((fmri_data), axis = 3)\n",
    "print(complete_movie_run_sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.3</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Eine Computeranimation: Auf einen schroffen Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.9</td>\n",
       "      <td>19.5</td>\n",
       "      <td>Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.6</td>\n",
       "      <td>25.8</td>\n",
       "      <td>In einer Stadt segelt sie Ã¼ber die Baumkronen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.7</td>\n",
       "      <td>36.7</td>\n",
       "      <td>Ein Robert-Zemeckis-Film: Tom Hanks als 'FORRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.7</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Die Feder fliegt ein paar Meter Ã¼ber den Wipfe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start   end                                               text\n",
       "0    1.3  10.5  Eine Computeranimation: Auf einen schroffen Be...\n",
       "1   15.9  19.5  Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe F...\n",
       "2   22.6  25.8  In einer Stadt segelt sie Ã¼ber die Baumkronen ...\n",
       "3   31.7  36.7  Ein Robert-Zemeckis-Film: Tom Hanks als 'FORRE...\n",
       "4   40.7  61.0  Die Feder fliegt ein paar Meter Ã¼ber den Wipfe..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv with annotations\n",
    "dialogue_file = \"C:/Users/lusim/OneDrive/Desktop/neurocomp/ds000113/stimuli/annotations/german_audio_description.csv\"\n",
    "text_data = pd.read_csv(dialogue_file, header=None)\n",
    "text_data.columns = ['start', 'end', 'text']\n",
    "texts = text_data['text']\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573/573 [00:07<00:00, 76.94it/s] \n"
     ]
    }
   ],
   "source": [
    "# nlp model deutsch\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "text_vectors = []\n",
    "\n",
    "for _, row in tqdm(text_data.iterrows(), total = text_data.shape[0]):\n",
    "    doc = nlp(row['text'])\n",
    "    text_vectors.append(doc.vector)\n",
    "    \n",
    "text_vectors = np.array(text_vectors)\n",
    "   \n",
    "# TR = 2s (tiempo de adquisicion) y 3599 muestras temporales.\n",
    "TS = 2\n",
    "fmri_times = np.arange(0, 3599*TS, TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_text_vectors = np.zeros((3599, text_vectors.shape[1]))  # Inicializar con ceros\n",
    "\n",
    "for start, end, vector in zip(text_data['start'], text_data['end'], text_vectors):\n",
    "    start_idx = int(np.floor(start / 2))\n",
    "    end_idx = int(np.ceil(end / 2))\n",
    "    for i in range(start_idx, end_idx):\n",
    "        if i < 3599:  # Asegurar que no exceda el nÃºmero de muestras de fMRI\n",
    "            aligned_text_vectors[i] = vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the affine from one of the original images (assuming they all have the same affine)\n",
    "affine = nib.load(fnames_data[0]).affine\n",
    "# Create a new NiBabel image\n",
    "complete_movie_run_img = nib.Nifti1Image(complete_movie_run_sub1, affine)\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 100) \n",
    "\n",
    "masker = masking.compute_epi_mask(complete_movie_run_img)\n",
    "masked_data = masking.apply_mask(complete_movie_run_img, masker)\n",
    "reduced_fmri = pca.fit_transform(masked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_fmri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajusto el modelo lineal\n",
    "reg = LinearRegression()\n",
    "reg.fit(reduced_fmri, aligned_text_vectors)\n",
    "prediction = reg.predict(reduced_fmri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo similitud entre \n",
    "# vectores de texto y las predicciones para cada sujeto\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "thresh = 0.8\n",
    "\n",
    "similarity = cosine_similarity(prediction, aligned_text_vectors)\n",
    "correct_predictions = np.sum(np.diag(similarity) >= thresh)\n",
    "total_predictions = similarity.shape[0]\n",
    "accuracy = correct_predictions / total_predictions * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.31203111975549\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_to_phrases(vectors, nlp, phrases, text_vectors):\n",
    "    texts = []\n",
    "    for vector in vectors:\n",
    "        # Calcular la similitud coseno con las frases originales\n",
    "        similarities = cosine_similarity([vector], text_vectors)[0]\n",
    "        # Encontrar la frase mÃ¡s similar\n",
    "        most_similar_index = np.argmax(similarities)\n",
    "        texts.append(phrases[most_similar_index])\n",
    "    return texts\n",
    "\n",
    "phrases = texts.tolist()\n",
    "predicted_phrases = vectors_to_phrases(prediction, nlp, phrases, text_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frases predichas:\n",
      "Eine Computeranimation: Auf einen schroffen Berg mit schneebedeckter Flanke fliegt eine Reihe Sterne zu. Sie bilden einen Kranz um den Gipfel: 'Paramount'.\n",
      "Eine Computeranimation: Auf einen schroffen Berg mit schneebedeckter Flanke fliegt eine Reihe Sterne zu. Sie bilden einen Kranz um den Gipfel: 'Paramount'.\n",
      "Eine Computeranimation: Auf einen schroffen Berg mit schneebedeckter Flanke fliegt eine Reihe Sterne zu. Sie bilden einen Kranz um den Gipfel: 'Paramount'.\n",
      "Eine Computeranimation: Auf einen schroffen Berg mit schneebedeckter Flanke fliegt eine Reihe Sterne zu. Sie bilden einen Kranz um den Gipfel: 'Paramount'.\n",
      "Eine Computeranimation: Auf einen schroffen Berg mit schneebedeckter Flanke fliegt eine Reihe Sterne zu. Sie bilden einen Kranz um den Gipfel: 'Paramount'.\n",
      "Eine Computeranimation: Auf einen schroffen Berg mit schneebedeckter Flanke fliegt eine Reihe Sterne zu. Sie bilden einen Kranz um den Gipfel: 'Paramount'.\n",
      "Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe Feder durch die Luft.\n",
      "Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe Feder durch die Luft.\n",
      "Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe Feder durch die Luft.\n",
      "Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe Feder durch die Luft.\n",
      "Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe Feder durch die Luft.\n",
      "Vor bewÃ¶lktem Himmel schwebt eine grau-weiÃŸe Feder durch die Luft.\n",
      "In einer Stadt segelt sie Ã¼ber die Baumkronen eines Parks.\n",
      "Eine Computeranimation: Auf einen schroffen Berg mit schneebedeckter Flanke fliegt eine Reihe Sterne zu. Sie bilden einen Kranz um den Gipfel: 'Paramount'.\n",
      "Ein Robert-Zemeckis-Film: Tom Hanks als 'FORREST GUMP'.\n",
      "Ein Robert-Zemeckis-Film: Tom Hanks als 'FORREST GUMP'.\n",
      "Ein Robert-Zemeckis-Film: Tom Hanks als 'FORREST GUMP'.\n",
      "Ein Robert-Zemeckis-Film: Tom Hanks als 'FORREST GUMP'.\n",
      "Die Feder fliegt ein paar Meter Ã¼ber den Wipfeln. Robin Wright als Jenny; Gary Sinise als Lieutenant Dan; Mykelti Williamson als Bubba und Sally Field als Missis Gump; Musik: Alan Silvestri; Drehbuch: Eric Roth; Nach dem gleichnamigen Roman von Winston Groom; Regie: Robert Zemeckis.\n",
      "Die Feder fliegt ein paar Meter Ã¼ber den Wipfeln. Robin Wright als Jenny; Gary Sinise als Lieutenant Dan; Mykelti Williamson als Bubba und Sally Field als Missis Gump; Musik: Alan Silvestri; Drehbuch: Eric Roth; Nach dem gleichnamigen Roman von Winston Groom; Regie: Robert Zemeckis.\n"
     ]
    }
   ],
   "source": [
    "# Imprimir algunas frases predichas\n",
    "print(\"Frases predichas:\")\n",
    "for phrase in predicted_phrases[:20]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 302, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1289, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1310, in _check_y\n    y = column_or_1d(y, warn=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1377, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (360, 96) instead.\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 302, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1289, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1310, in _check_y\n    y = column_or_1d(y, warn=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1377, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (361, 96) instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nilearn\\decoding\\searchlight.py\", line 192, in _group_iter_search_light\n    cross_val_score(estimator, X[:, row], y, cv=cv, n_jobs=1, **kwargs)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 712, in cross_val_score\n    cv_results = cross_validate(\n                 ^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 443, in cross_validate\n    _warn_or_raise_about_fit_failures(results, error_score)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 529, in _warn_or_raise_about_fit_failures\n    raise ValueError(all_fits_failed_message)\nValueError: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 302, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1289, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1310, in _check_y\n    y = column_or_1d(y, warn=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1377, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (360, 96) instead.\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 302, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1289, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1310, in _check_y\n    y = column_or_1d(y, warn=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1377, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (361, 96) instead.\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 26\u001b[0m\n\u001b[0;32m     17\u001b[0m searchlight \u001b[38;5;241m=\u001b[39m SearchLight(\n\u001b[0;32m     18\u001b[0m     mask_img,\n\u001b[0;32m     19\u001b[0m     process_mask_img \u001b[38;5;241m=\u001b[39m mask_img,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Usar todos los nÃºcleos disponibles\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Aplicar searchlight\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43msearchlight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned_text_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Visualizar los resultados del searchlight\u001b[39;00m\n\u001b[0;32m     29\u001b[0m searchlight_img \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mnew_img_like(mask_img, searchlight\u001b[38;5;241m.\u001b[39mscores_)\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nilearn\\decoding\\searchlight.py:341\u001b[0m, in \u001b[0;36mSearchLight.fit\u001b[1;34m(self, imgs, y, groups)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    339\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m ESTIMATOR_CATALOG[estimator]()\n\u001b[1;32m--> 341\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43msearch_light\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m scores_3D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(process_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    353\u001b[0m scores_3D[process_mask] \u001b[38;5;241m=\u001b[39m scores\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nilearn\\decoding\\searchlight.py:88\u001b[0m, in \u001b[0;36msearch_light\u001b[1;34m(X, y, estimator, A, groups, scoring, cv, n_jobs, verbose)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():  \u001b[38;5;66;03m# might not converge\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, ConvergenceWarning)\n\u001b[1;32m---> 88\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_group_iter_search_light\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m            \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlist_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(scores)\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 302, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1289, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1310, in _check_y\n    y = column_or_1d(y, warn=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1377, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (360, 96) instead.\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py\", line 302, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1289, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1310, in _check_y\n    y = column_or_1d(y, warn=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\lusim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1377, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (361, 96) instead.\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar datos de fMRI\n",
    "\n",
    "fmri_img = fmri_imgs[0]\n",
    "\n",
    "masker = NiftiMasker(mask_img=masking.compute_brain_mask(fmri_img))\n",
    "fmri_data = masker.fit_transform(fmri_img)\n",
    "\n",
    "# FunciÃ³n de regresiÃ³n para searchlight\n",
    "def searchlight_regression(X, y):\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, y)\n",
    "    return reg.score(X, y)\n",
    "\n",
    "# Configurar y ejecutar el searchlight\n",
    "mask_img = masker.mask_img_\n",
    "searchlight = SearchLight(\n",
    "    mask_img,\n",
    "    process_mask_img = mask_img,\n",
    "    radius=5,  # Radio del searchlight en mm\n",
    "    scoring = searchlight_regression,\n",
    "    n_jobs=-1  # Usar todos los nÃºcleos disponibles\n",
    ")\n",
    "\n",
    "# Aplicar searchlight\n",
    "searchlight.fit(fmri_img, aligned_text_vectors)\n",
    "\n",
    "# Visualizar los resultados del searchlight\n",
    "searchlight_img = image.new_img_like(mask_img, searchlight.scores_)\n",
    "plotting.plot_stat_map(\n",
    "    searchlight_img, bg_img=fmri_img, title=\"Searchlight Regression Scores\"\n",
    ")\n",
    "plotting.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
